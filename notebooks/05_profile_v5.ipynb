{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedGroupKFold, GroupKFold\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from pptoolbox.platform.preprocessing import preprocess_v2\n",
    "from pptoolbox.platform.automl_v4 import AutoML_v4\n",
    "from pptoolbox.platform.automl_v4.pipelines import META_REGRESSION_PIPELINES\n",
    "\n",
    "import os\n",
    "# import sys\n",
    "# sys.path.append(\"../src\")\n",
    "# from utils.pipelines_meta import META_REGRESSION_PIPELINES # Shifted into pptoolbox 1.3.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping meta utils here. \n",
    "# Algorithm design still undergoing testing. not yet stable for pptoolbox?\n",
    "import warnings\n",
    "from typing import Literal, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import BaseCrossValidator, cross_val_predict\n",
    "\n",
    "\n",
    "AUTOML_MODE_OPTIONS = [\"classify\", \"profile\"]\n",
    "\n",
    "def create_meta_datasets_splits(\n",
    "    estimators: list[list[Pipeline]],\n",
    "    datasets: list[pd.DataFrame],\n",
    "    y: pd.Series,\n",
    "    groups: np.ndarray,\n",
    "    outer_cv: BaseCrossValidator,\n",
    "    inner_cv: BaseCrossValidator,\n",
    "    mode: Literal[AUTOML_MODE_OPTIONS] = \"classify\",\n",
    ") -> tuple[pd.DataFrame, pd.Series, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Custom pre-processing for meta-model cross validation flow\n",
    "    This flow assumes we maintain the same split indexes across all hyperparams to be tested.\n",
    "    Because of this, we can pre-calculate the features being input to meta-model\n",
    "    Instead of re-fitting the same estimator at each fold\n",
    "    \"\"\"\n",
    "    if mode not in AUTOML_MODE_OPTIONS:\n",
    "        raise ValueError(f\"Unexpected mode: {mode}. Expected {AUTOML_MODE_OPTIONS}\")\n",
    "    if len(datasets) == 1:\n",
    "        raise ValueError(\n",
    "            \"This is meant for data fusion. Check your input is a List of pandas.DataFrame\"\n",
    "        )\n",
    "\n",
    "    for X in datasets[1:]:\n",
    "        if len(X) != len(datasets[0]):\n",
    "            raise IndexError(\n",
    "                f\"Lengths of input datasets do not match ({datasets[0].shape[0]}), ({X.shape[0]})\"\n",
    "            )\n",
    "\n",
    "    for i, estimator_list in enumerate(estimators):\n",
    "        if type(estimator_list) != list:\n",
    "            if type(estimator_list) == Pipeline:\n",
    "                warnings.warn(\"Single estimator detected. Converting to list\")\n",
    "                estimators[i] = [estimator_list]\n",
    "            else:\n",
    "                raise ValueError(\"estimators should be a list of lists of Pipelines\")\n",
    "\n",
    "    if len(datasets) != len(estimators):\n",
    "        raise ValueError(\n",
    "            \"Length of datasets and estimators do not match. Check your inputs\"\n",
    "        )\n",
    "\n",
    "    if mode == \"classify\":\n",
    "        n_classes = len(np.unique(y))\n",
    "    else:\n",
    "        n_classes = 1\n",
    "\n",
    "    n = len(datasets[0])\n",
    "    m_list = [0] + [len(estimator_list) for estimator_list in estimators]\n",
    "    m = sum(m_list) * n_classes\n",
    "    k = outer_cv.get_n_splits()\n",
    "\n",
    "    new_indices = np.concatenate([datasets[0].index] * k)\n",
    "    meta_X = np.zeros((n * k, m))\n",
    "    meta_y = pd.Series(np.concatenate([y] * k), index=new_indices)\n",
    "    splits_idx = np.zeros((n * k, 2))\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(\n",
    "        outer_cv.split(datasets[0], y, groups=groups)\n",
    "    ):\n",
    "        y_train = y.iloc[train_idx]\n",
    "        splits_idx[n * fold_idx : n * (fold_idx + 1), 0] = fold_idx\n",
    "        splits_idx[test_idx + (n * fold_idx), 1] = 1\n",
    "\n",
    "        for i in range(len(datasets)):\n",
    "            X = datasets[i]\n",
    "            for j, estimator in enumerate(estimators[i]):\n",
    "                X_train = X.iloc[train_idx]\n",
    "                X_test = X.iloc[test_idx]\n",
    "\n",
    "                if mode == \"classify\":\n",
    "                    # Classification Flow\n",
    "                    preds_train = cross_val_predict(\n",
    "                        estimator,\n",
    "                        X_train,\n",
    "                        y_train,\n",
    "                        groups=X_train.index,\n",
    "                        cv=inner_cv,\n",
    "                        method=\"predict_proba\",\n",
    "                    )\n",
    "                    estimator.fit(X_train, y_train)\n",
    "                    preds_test = estimator.predict_proba(X_test)\n",
    "                else:\n",
    "                    # Regression Flow\n",
    "                    preds_train = cross_val_predict(\n",
    "                        estimator,\n",
    "                        X_train,\n",
    "                        y_train,\n",
    "                        groups=X_train.index,\n",
    "                        cv=inner_cv,\n",
    "                        method=\"predict\",\n",
    "                    )\n",
    "                    estimator.fit(X_train, y_train)\n",
    "                    preds_test = estimator.predict(X_test)\n",
    "\n",
    "                meta_X[\n",
    "                    train_idx + n * fold_idx,\n",
    "                    n_classes * (j + m_list[i]) : n_classes * (j + 1 + m_list[i]),\n",
    "                ] = preds_train.reshape(X_train.shape[0], n_classes)\n",
    "                meta_X[\n",
    "                    test_idx + n * fold_idx,\n",
    "                    n_classes * (j + m_list[i]) : n_classes * (j + 1 + m_list[i]),\n",
    "                ] = preds_test.reshape(X_test.shape[0], n_classes)\n",
    "\n",
    "    meta_X = pd.DataFrame(meta_X, index=new_indices)\n",
    "\n",
    "    return meta_X, meta_y, splits_idx\n",
    "\n",
    "\n",
    "class CustomMetaSplitter(BaseCrossValidator):\n",
    "    def __init__(self, split_idx: np.ndarray):\n",
    "        self.splits = split_idx\n",
    "        self.n_splits = len(np.unique(split_idx[:, 0]))\n",
    "\n",
    "    def get_n_splits(\n",
    "        self, X: Optional = None, y: Optional = None, groups: Optional = None\n",
    "    ) -> int:\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X: Optional = None, y: Optional = None, groups: Optional = None):\n",
    "        for i in range(self.n_splits):\n",
    "            train_idx = np.where((self.splits[:, 0] == i) & (self.splits[:, 1] == 0))[0]\n",
    "            test_idx = np.where((self.splits[:, 0] == i) & (self.splits[:, 1] == 1))[0]\n",
    "            yield train_idx, test_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_trainer(task_type ='classify', log=False, experiment_name='', mlflow_uri=None, *args, **kwargs):\n",
    "    trainer = AutoML_v4(task_type = task_type, log = log, *args, **kwargs, exploration_runs=200)\n",
    "    if log == True:\n",
    "        trainer.set_log_config(\n",
    "            experiment_name = f\"{experiment_name}\",\n",
    "            mlflow_uri = mlflow_uri\n",
    "        )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metafeatures_profile(datasets, estimators, y, cv=GroupKFold(n_splits=3)):\n",
    "    n_classes = 1\n",
    "    for i, estimator_list in enumerate(estimators):\n",
    "        if type(estimator_list) != list:\n",
    "            if type(estimator_list) == Pipeline:\n",
    "                # warnings.warn(\"Single estimator detected. Converting to list\")\n",
    "                estimators[i] = [estimator_list]\n",
    "            else:\n",
    "                raise ValueError(\"estimators should be a list of lists of Pipelines\")\n",
    "    n = len(datasets[0])\n",
    "    m_list = [0] + [len(estimator_list) for estimator_list in estimators]\n",
    "    m = sum(m_list) * n_classes\n",
    "\n",
    "    X_meta = np.zeros([n,m])\n",
    "    for i in range(len(datasets)):\n",
    "        X = datasets[i]\n",
    "        for j, estimator in enumerate(estimators[i]):\n",
    "            y_preds = cross_val_predict(estimator, X, y, groups=X.index, cv=cv, method='predict')\n",
    "\n",
    "            X_meta[\n",
    "                :,\n",
    "                n_classes * (j + m_list[i]) : n_classes * (j + 1 + m_list[i]),\n",
    "            ] = y_preds.reshape(n, n_classes)\n",
    "            pass\n",
    "\n",
    "    return X_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Combined file simulating platform utils file.\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pickle as pkl\n",
    "from typing import Annotated, List, Tuple, Optional\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pptoolbox.platform.automl_v4 import cross_group_predict\n",
    "from pptoolbox.platform.automl_v4 import AutoML_v4\n",
    "\n",
    "# Dump to file\n",
    "def model_dump_regression(trainer: AutoML_v4, estimator, y: pd.Series, output_dir: str, suffix: str=\"\"):\n",
    "    # Dumps with this function will be done on a single label basis\n",
    "    # This is only a partial dump. Final model needs to be selected by higher level function after reviewing metrics\n",
    "    label = y.name\n",
    "\n",
    "    pkl.dump(\n",
    "        estimator, \n",
    "        open(os.path.join(output_dir, f\"tasteprofile_model_{label}{suffix}.pkl\"), \"wb\")\n",
    "    )\n",
    "    pkl.dump(\n",
    "        trainer, \n",
    "        open(os.path.join(output_dir, f\"trainer_{label}{suffix}.pkl\"), \"wb\")\n",
    "    )\n",
    "\n",
    "    # Get CV metrics from AutoML trainer\n",
    "    metrics = generate_regression_metrics(trainer, y)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Get Regression CV metrics from AutoML trainer\n",
    "def generate_regression_metrics(trainer, y):\n",
    "    best_trial = trainer.get_best_trial()\n",
    "    best_cv_results = best_trial[\"result\"][\"cv_results\"]\n",
    "\n",
    "    pmae_range = abs(y.max() - y.min())\n",
    "    cv_mae = -best_cv_results[\"test_neg_mean_absolute_error\"]\n",
    "    cv_pmae = (cv_mae * 100) / pmae_range\n",
    "\n",
    "    metrics = {\n",
    "        \"model\": trainer.get_pipeline_name(), \n",
    "        \"param\": y.name,\n",
    "        \"r2\": best_cv_results[\"test_r2\"],\n",
    "        \"mae\": -best_cv_results[\"test_neg_mean_absolute_error\"],\n",
    "        \"mse\": -best_cv_results[\"test_neg_mean_squared_error\"],\n",
    "        \"rmse\": -best_cv_results[\"test_neg_root_mean_squared_error\"],\n",
    "        \"max_error\": best_cv_results[\"test_max_error\"],\n",
    "        \"pmae\": cv_pmae,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "from pptoolbox.visualization.reg_visualizers import get_reg_figures\n",
    "def dump_images_regression(estimator, X:pd.DataFrame, y:pd.Series, output_dir:str, suffix:str='None'):\n",
    "\n",
    "    fig1, fig2 = get_reg_figures(\n",
    "        model=estimator,\n",
    "        Xtrain=X,\n",
    "        ytrain=y,\n",
    "        test=False,\n",
    "    )\n",
    "    im_dir = Path(output_dir / 'images')\n",
    "    im_dir.mkdir(exist_ok=True)\n",
    "    label = y.name\n",
    "    \n",
    "    # Prediction error fig (Actual vs Predicted)\n",
    "    fig1.update_layout(title=suffix)\n",
    "    output_path = Path(im_dir / f\"prederror_{label}{suffix}.html\")\n",
    "    fig1.write_html(output_path)\n",
    "\n",
    "    # Residuals fig\n",
    "    fig2.update_layout(title=suffix)\n",
    "    output_path = Path(im_dir / f\"residuals_{label}{suffix}.html\")\n",
    "    fig2.write_html(output_path)\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise configs\n",
    "sql_response_file = Path(\"../data/processed/input.csv\")\n",
    "labels_file = Path(\"../data/processed/label.csv\")\n",
    "# output_dir = Path(\"../model/predictions_profile\")\n",
    "experiment_name = \"ketchup profile\"\n",
    "\n",
    "# Rename the qmini, qneo columns to align with automl_orca hardcoded names\n",
    "# mapper = {\n",
    "#     \"calc_data_mini\": \"raw_data_mini\",\n",
    "#     \"calc_data_neo\": \"raw_data_neo\",\n",
    "# }\n",
    "# input_df = input_df.rename(mapper=mapper, axis=1)\n",
    "\n",
    "input_df = pd.read_csv(sql_response_file) \n",
    "labels_df = pd.read_csv(labels_file)\n",
    "\n",
    "labels_df = labels_df[['lot_id', 'price per weight',  'Days to Expiry', 'avg mid','salt (mg)','carbohydrates (g)','protein (g)']]\n",
    "\n",
    "X_visnir, X_exnir, y, datatype = preprocess_v2(input_df, labels_df)\n",
    "\n",
    "N_SPLITS = 3\n",
    "DEFAULT_CV = GroupKFold(n_splits=N_SPLITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(y.columns)[0]\n",
    "input_y = y[label] # Show example for single column only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run visnir segment\n",
    "trainer = initialise_trainer(task_type='profile', log=False)\n",
    "trainer.fit(X_visnir, input_y, kfold=DEFAULT_CV)\n",
    "score_visnir = trainer.get_best_performance()\n",
    "best_visnir = trainer.get_pipeline()\n",
    "metrics_visnir = generate_regression_metrics(trainer, input_y)\n",
    "trainer_visnir = trainer\n",
    "base_visnir = [best_visnir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run exnir segment\n",
    "trainer = initialise_trainer(task_type='profile', log=False)\n",
    "trainer.fit(X_exnir, input_y, kfold=DEFAULT_CV)\n",
    "score_exnir = trainer.get_best_performance()\n",
    "best_exnir = trainer.get_pipeline()\n",
    "metrics_exnir = generate_regression_metrics(trainer, input_y)\n",
    "trainer_exnir = trainer\n",
    "base_exnir = [best_exnir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for metamodel segment\n",
    "datasets = [X_visnir, X_exnir]\n",
    "estimators = [base_visnir, base_exnir] \n",
    "X_meta, y_meta, split_idx = create_meta_datasets_splits(\n",
    "    estimators=estimators, \n",
    "    datasets=datasets, \n",
    "    y=input_y, \n",
    "    groups=X_visnir.index, \n",
    "    outer_cv=DEFAULT_CV, \n",
    "    inner_cv=DEFAULT_CV,\n",
    "    mode='profile',\n",
    ")\n",
    "\n",
    "# Run metamodel segment\n",
    "cv_meta = CustomMetaSplitter(split_idx = split_idx)\n",
    "trainer = initialise_trainer(task_type='profile', log=False)\n",
    "trainer.fit(X_meta, y_meta, kfold=cv_meta, search_space=META_REGRESSION_PIPELINES, groups=X_meta.index)\n",
    "score_meta = trainer.get_best_performance()\n",
    "metrics_meta = generate_regression_metrics(trainer, input_y)\n",
    "\n",
    "X_meta_final = generate_metafeatures_profile(datasets, estimators, y=input_y)\n",
    "best_meta = trainer.get_pipeline()\n",
    "best_meta.fit(X_meta_final, input_y)\n",
    "trainer_meta = trainer\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review metrics without dumping\n",
    "import pprint\n",
    "print(score_visnir, score_exnir, score_meta)\n",
    "\n",
    "print()\n",
    "print('visnir')\n",
    "pprint.pprint(metrics_visnir)\n",
    "\n",
    "print()\n",
    "print('exnir')\n",
    "pprint.pprint(metrics_exnir)\n",
    "\n",
    "print()\n",
    "print('metamodel')\n",
    "pprint.pprint(metrics_meta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, fig2 = get_reg_figures(\n",
    "    model=best_visnir,\n",
    "    Xtrain=X_visnir,\n",
    "    ytrain=input_y,\n",
    "    test=False,\n",
    ")\n",
    "\n",
    "# Prediction error fig (Actual vs Predicted)\n",
    "fig1.update_layout(title='visnir')\n",
    "fig1.show()\n",
    "\n",
    "# Residuals fig\n",
    "fig2.update_layout(title='visnir')\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, fig2 = get_reg_figures(\n",
    "    model=best_exnir,\n",
    "    Xtrain=X_exnir,\n",
    "    ytrain=input_y,\n",
    "    test=False,\n",
    ")\n",
    "\n",
    "# Prediction error fig (Actual vs Predicted)\n",
    "fig1.update_layout(title='exnir')\n",
    "fig1.show()\n",
    "\n",
    "# Residuals fig\n",
    "fig2.update_layout(title='exnir')\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, fig2 = get_reg_figures(\n",
    "    model=best_meta,\n",
    "    Xtrain=X_meta_final,\n",
    "    ytrain=input_y,\n",
    "    test=False,\n",
    ")\n",
    "\n",
    "# Prediction error fig (Actual vs Predicted)\n",
    "fig1.update_layout(title='meta')\n",
    "fig1.show()\n",
    "\n",
    "# Residuals fig\n",
    "fig2.update_layout(title='meta')\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Single\n",
    "# Dump metrics to folder\n",
    "output_dir = Path(\"../tests/testdata/predictions_profile/\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "_ = model_dump_regression(\n",
    "    trainer=trainer_visnir, \n",
    "    estimator=best_visnir, \n",
    "    y=input_y, output_dir=output_dir, suffix=\"_visnir\",\n",
    ")\n",
    "\n",
    "_ = model_dump_regression(\n",
    "    trainer=trainer_exnir, \n",
    "    estimator=best_exnir, \n",
    "    y=input_y, output_dir=output_dir, suffix=\"_exnir\",\n",
    ")\n",
    "\n",
    "_ = model_dump_regression(\n",
    "    trainer=trainer_meta, \n",
    "    estimator=best_meta, \n",
    "    y=input_y, output_dir=output_dir, suffix=\"_meta\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Loop with dumps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptoolbox.platform.utils import export_pi_df, initialize_pi_df\n",
    "\n",
    "# Export EVERYTHING, images included as plotly html\n",
    "# Dump metrics to folder\n",
    "output_dir = Path(\"../model/predictions_profile/\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price per weight', 'Days to Expiry', 'avg mid', 'salt (mg)',\n",
       "       'carbohydrates (g)', 'protein (g)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price per weight\n",
      "100%|██████████| 350/350 [02:15<00:00,  2.57trial/s, best loss: 101.87539117857388]\n",
      "100%|██████████| 350/350 [01:35<00:00,  3.67trial/s, best loss: 92.4137002740031] \n",
      "100%|██████████| 350/350 [02:17<00:00,  2.54trial/s, best loss: 126.76056728944715]\n",
      "Days to Expiry\n",
      "100%|██████████| 350/350 [02:21<00:00,  2.47trial/s, best loss: 22521.238758727857] \n",
      " 99%|█████████▉| 348/350 [02:56<00:01,  1.97trial/s, best loss: 32277.80421860018] \n",
      " 82%|████████▏ | 287/350 [01:58<00:25,  2.43trial/s, best loss: 30501.396802709394]\n",
      "avg mid\n",
      " 87%|████████▋ | 303/350 [01:57<00:18,  2.58trial/s, best loss: 12.46116558140315] \n",
      " 88%|████████▊ | 308/350 [01:32<00:12,  3.32trial/s, best loss: 12.829784038648102]\n",
      "100%|██████████| 350/350 [02:32<00:00,  2.29trial/s, best loss: 14.513502994076136]\n",
      "salt (mg)\n",
      "100%|██████████| 350/350 [02:07<00:00,  2.74trial/s, best loss: 649458.6167363699]\n",
      " 82%|████████▏ | 287/350 [01:17<00:16,  3.72trial/s, best loss: 518633.13370096]  \n",
      " 88%|████████▊ | 307/350 [02:20<00:19,  2.18trial/s, best loss: 683082.5825558343]\n",
      "carbohydrates (g)\n",
      "100%|██████████| 350/350 [02:38<00:00,  2.21trial/s, best loss: 28.231184865491336]\n",
      "100%|██████████| 350/350 [02:24<00:00,  2.43trial/s, best loss: 22.852022858874726]\n",
      " 83%|████████▎ | 289/350 [02:29<00:31,  1.93trial/s, best loss: 61.6378232524456]\n",
      "protein (g)\n",
      "100%|██████████| 350/350 [02:25<00:00,  2.41trial/s, best loss: 0.5623147149144783]\n",
      "100%|██████████| 350/350 [02:18<00:00,  2.53trial/s, best loss: 0.5377129138415909]\n",
      "100%|██████████| 350/350 [02:57<00:00,  1.97trial/s, best loss: 0.6156630304981954]\n"
     ]
    }
   ],
   "source": [
    "# this flow considers each column in y_DataFrame as regression labels\n",
    "# Will loop through each column and generate outputs for each\n",
    "\n",
    "# Will also generate a consolidated metrics file for each data type (visnir, exnir, meta)\n",
    "\n",
    "metrics_visnir = []\n",
    "metrics_exnir = []\n",
    "metrics_meta = []\n",
    "\n",
    "metrics_best = []\n",
    "labels = list(y.columns)\n",
    "for label in labels:\n",
    "    input_y = y[label]\n",
    "\n",
    "    print (label)\n",
    "\n",
    "    # Run visnir segment\n",
    "    trainer = initialise_trainer(task_type='profile', log=False)\n",
    "    trainer.fit(X_visnir, input_y, kfold=DEFAULT_CV)\n",
    "    score_visnir = trainer.get_best_performance()\n",
    "    best_visnir = trainer.get_pipeline()\n",
    "    base_visnir = [best_visnir]\n",
    "\n",
    "    metric_visnir = model_dump_regression(\n",
    "        trainer=trainer, \n",
    "        estimator=trainer.get_pipeline(), \n",
    "        y=input_y, output_dir=output_dir, suffix=\"_visnir\",\n",
    "    )\n",
    "    metrics_visnir.append(metric_visnir)\n",
    "\n",
    "    dump_images_regression(\n",
    "        estimator=trainer.get_pipeline(), \n",
    "        X=X_visnir, \n",
    "        y=input_y, output_dir=output_dir, suffix=\"_visnir\",\n",
    "    )\n",
    "\n",
    "    # Run exnir segment\n",
    "    trainer = initialise_trainer(task_type='profile', log=False)\n",
    "    trainer.fit(X_exnir, input_y, kfold=DEFAULT_CV)\n",
    "    score_exnir = trainer.get_best_performance()\n",
    "    best_exnir = trainer.get_pipeline()\n",
    "    base_exnir = [best_exnir]\n",
    "\n",
    "    metric_exnir = model_dump_regression(\n",
    "        trainer=trainer, \n",
    "        estimator=trainer.get_pipeline(), \n",
    "        y=input_y, output_dir=output_dir, suffix=\"_exnir\",\n",
    "    )\n",
    "    metrics_exnir.append(metric_exnir)\n",
    "\n",
    "    dump_images_regression(\n",
    "        estimator=trainer.get_pipeline(), \n",
    "        X=X_exnir, \n",
    "        y=input_y, output_dir=output_dir, suffix=\"_exnir\",\n",
    "    )\n",
    "\n",
    "    # Prepare dataset for metamodel segment\n",
    "    datasets = [X_visnir, X_exnir]\n",
    "    estimators = [base_visnir, base_exnir] \n",
    "    X_meta, y_meta, split_idx = create_meta_datasets_splits(\n",
    "        estimators=estimators, \n",
    "        datasets=datasets, \n",
    "        y=input_y, \n",
    "        groups=X_visnir.index, \n",
    "        outer_cv=DEFAULT_CV, \n",
    "        inner_cv=DEFAULT_CV,\n",
    "        mode='profile',\n",
    "    )\n",
    "\n",
    "    # Run metamodel segment\n",
    "    cv_meta = CustomMetaSplitter(split_idx = split_idx)\n",
    "    trainer = initialise_trainer(task_type='profile', log=False)\n",
    "    trainer.fit(X_meta, y_meta, kfold=cv_meta, search_space=META_REGRESSION_PIPELINES, groups=X_meta.index)\n",
    "    score_meta = trainer.get_best_performance()\n",
    "\n",
    "    X_meta_final = generate_metafeatures_profile(datasets, estimators, y=input_y)\n",
    "    best_meta = trainer.get_pipeline()\n",
    "    best_meta.fit(X_meta_final, input_y)\n",
    "\n",
    "    metric_meta = model_dump_regression(\n",
    "        trainer=trainer, \n",
    "        estimator=trainer.get_pipeline(), \n",
    "        y=input_y, output_dir=output_dir, suffix=\"_meta\",\n",
    "    )\n",
    "    metrics_meta.append(metric_meta)\n",
    "\n",
    "    dump_images_regression(\n",
    "        estimator=trainer.get_pipeline(), \n",
    "        X=X_meta_final, \n",
    "        y=input_y, output_dir=output_dir, suffix=\"_meta\",\n",
    "    )\n",
    "\n",
    "    best_model = np.argmax(np.array([score_visnir, score_exnir, score_meta]))\n",
    "    # argmax, Most positive neg-MSE wins\n",
    "    metric_options = {\n",
    "        0: metric_visnir,\n",
    "        1: metric_exnir,\n",
    "        2: metric_meta,\n",
    "    }\n",
    "    metrics_best.append(metric_options[best_model])\n",
    "    continue\n",
    "\n",
    "# Export pi_df and common metrics for best models\n",
    "pi_df = initialize_pi_df(labels)\n",
    "export_pi_df(pi_df, output_dir)\n",
    "metrics_df = pd.DataFrame(metrics_best)\n",
    "metrics_df.to_csv(os.path.join(output_dir, \"metrics.csv\"), index=False) \n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_visnir)\n",
    "metrics_df.to_csv(os.path.join(output_dir, \"metrics_visnir.csv\"), index=False)\n",
    "metrics_df = pd.DataFrame(metrics_exnir)\n",
    "metrics_df.to_csv(os.path.join(output_dir, \"metrics_exnir.csv\"), index=False)\n",
    "metrics_df = pd.DataFrame(metrics_meta)\n",
    "metrics_df.to_csv(os.path.join(output_dir, \"metrics_meta.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ketchup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
